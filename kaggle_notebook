{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":119874,"databundleVersionId":14372465,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0) Problem Statement","metadata":{}},{"cell_type":"markdown","source":"Develop a PyTorch model that predicts both gender (binary classification) and age (regression) from facial images. The goal is to design, train, and evaluate models effectively. Train two models on the provided dataset. - Simple CNN from scratch - Finetuned from pretrained models.\n\n","metadata":{}},{"cell_type":"markdown","source":"# 1) Import Important Libraries for DL","metadata":{}},{"cell_type":"code","source":"!pip install torch --quiet\n!pip install torchvision --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:31:28.106396Z","iopub.execute_input":"2025-11-10T07:31:28.107139Z","iopub.status.idle":"2025-11-10T07:32:44.311504Z","shell.execute_reply.started":"2025-11-10T07:31:28.107103Z","shell.execute_reply":"2025-11-10T07:32:44.310556Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 2) Importing Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nbase_path = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset\"\n\ntrain = pd.read_csv(f\"{base_path}/train.csv\")\ntest = pd.read_csv(f\"{base_path}/test.csv\")\n\nprint(train.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:32:44.312982Z","iopub.execute_input":"2025-11-10T07:32:44.313608Z","iopub.status.idle":"2025-11-10T07:32:44.656532Z","shell.execute_reply.started":"2025-11-10T07:32:44.313572Z","shell.execute_reply":"2025-11-10T07:32:44.655904Z"}},"outputs":[{"name":"stdout","text":"   id        full_path  gender  age\n0   0  train/00000.jpg       1   66\n1   1  train/00001.jpg       1   53\n2   2  train/00002.jpg       1   20\n3   3  train/00003.jpg       1   32\n4   4  train/00004.jpg       1   21\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(train['gender'].value_counts())\nprint(train['age'].describe())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:32:44.657175Z","iopub.execute_input":"2025-11-10T07:32:44.657374Z","iopub.status.idle":"2025-11-10T07:32:44.673378Z","shell.execute_reply.started":"2025-11-10T07:32:44.657357Z","shell.execute_reply":"2025-11-10T07:32:44.672379Z"}},"outputs":[{"name":"stdout","text":"gender\n1    26084\n0     8624\nName: count, dtype: int64\ncount    34708.000000\nmean        37.346692\nstd         16.587891\nmin          1.000000\n25%         25.000000\n50%         32.000000\n75%         48.000000\nmax        100.000000\nName: age, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 4) Transform","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:32:44.675102Z","iopub.execute_input":"2025-11-10T07:32:44.675295Z","iopub.status.idle":"2025-11-10T07:32:50.759493Z","shell.execute_reply.started":"2025-11-10T07:32:44.675279Z","shell.execute_reply":"2025-11-10T07:32:50.758902Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 5) All Images to Tensor","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom PIL import Image\n\nX, age, gender = [], [], []\n\nfor i, row in train.iterrows():\n    img_path = os.path.join(base_path, row['full_path'])\n    img = Image.open(img_path).convert(\"RGB\")\n    img = transform(img)\n    X.append(img)\n    age.append(row['age'])\n    gender.append(row['gender'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:32:50.760169Z","iopub.execute_input":"2025-11-10T07:32:50.760433Z","iopub.status.idle":"2025-11-10T07:35:30.394632Z","shell.execute_reply.started":"2025-11-10T07:32:50.760417Z","shell.execute_reply":"2025-11-10T07:35:30.393790Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\n\nX = torch.stack(X)\nage = torch.tensor(age, dtype=torch.float32)\ngender = torch.tensor(gender, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:30.395596Z","iopub.execute_input":"2025-11-10T07:35:30.395877Z","iopub.status.idle":"2025-11-10T07:35:31.732588Z","shell.execute_reply.started":"2025-11-10T07:35:30.395847Z","shell.execute_reply":"2025-11-10T07:35:31.731680Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 6) Preprocessings","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ndataset = TensorDataset(X, age, gender)\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:31.733480Z","iopub.execute_input":"2025-11-10T07:35:31.733737Z","iopub.status.idle":"2025-11-10T07:35:31.737668Z","shell.execute_reply.started":"2025-11-10T07:35:31.733712Z","shell.execute_reply":"2025-11-10T07:35:31.736957Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        # Convolutional layers\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        # Fully connected (shared) part\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 16 * 16, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n\n        # Two heads for multi-task output\n        self.age_head = nn.Linear(128, 1)\n        self.gender_head = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc(x)\n        age = torch.sigmoid(self.age_head(x)) * 100.0  # normalize age output\n        gender = self.gender_head(x)  # raw logits for binary classification\n        return age.squeeze(1), gender.squeeze(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:38:53.168060Z","iopub.execute_input":"2025-11-10T07:38:53.168934Z","iopub.status.idle":"2025-11-10T07:38:53.239937Z","shell.execute_reply.started":"2025-11-10T07:38:53.168907Z","shell.execute_reply":"2025-11-10T07:38:53.239273Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# 7) Instantiate","metadata":{}},{"cell_type":"code","source":"model = SimpleCNN().to(device)\n\nmse = nn.MSELoss()\nbce = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4) \n\nfrom torch.utils.data import TensorDataset, DataLoader\ndataset = TensorDataset(X, age, gender)\nloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:38:58.544394Z","iopub.execute_input":"2025-11-10T07:38:58.545143Z","iopub.status.idle":"2025-11-10T07:38:58.727344Z","shell.execute_reply.started":"2025-11-10T07:38:58.545119Z","shell.execute_reply":"2025-11-10T07:38:58.726783Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 8) Training Loop with Trackio","metadata":{}},{"cell_type":"code","source":"!pip install trackio --quiet\nimport trackio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:39:05.273855Z","iopub.execute_input":"2025-11-10T07:39:05.274557Z","iopub.status.idle":"2025-11-10T07:39:39.870071Z","shell.execute_reply.started":"2025-11-10T07:39:05.274523Z","shell.execute_reply":"2025-11-10T07:39:39.869415Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.0/875.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trackio.init(project=\"my-training\", config={\n            \"epochs\": 10,\n            \"learning_rate\": 0.001,\n            \"batch_size\": 64\n        })\n\nfor epoch in range(150): \n    model.train()\n    total_loss = 0.0\n\n    for images, ages, genders in loader:\n        images, ages, genders = images.to(device), ages.to(device), genders.to(device)\n\n        optimizer.zero_grad()\n\n        age_pred, gender_pred = model(images)\n\n        loss_age = mse(age_pred, ages)\n        loss_gender = bce(gender_pred, genders)\n        loss = 0.9 * loss_age + 0.1 * loss_gender  \n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(loader)\n    trackio.log({\n                \"epoch\": epoch,\n                \"av_loss\": avg_loss,\n                \n            })\n    print(f\"Epoch [{epoch+1}/100] — Loss: {avg_loss:.4f}\")\ntrackio.finish()\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:39:47.844025Z","iopub.execute_input":"2025-11-10T07:39:47.844620Z"}},"outputs":[{"name":"stdout","text":"* Running on public URL: https://8fa3bda1ef8b2e685d.gradio.live\n* Trackio project initialized: my-training\n* Trackio metrics logged to: /root/.cache/huggingface/trackio\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://8fa3bda1ef8b2e685d.gradio.live/?project=my-training&write_token=dPhpl5vcK1kitjxCIBmtC-I_D47evG2sQjA9p2Ey7XY\" width=\"100%\" height=\"1000px\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"* Created new run: dainty-sunset-0\nEpoch [1/100] — Loss: 194.1313\nEpoch [2/100] — Loss: 166.0070\nEpoch [3/100] — Loss: 152.5626\nEpoch [4/100] — Loss: 143.2407\nEpoch [5/100] — Loss: 134.4991\nEpoch [6/100] — Loss: 126.7083\nEpoch [7/100] — Loss: 120.2748\nEpoch [8/100] — Loss: 114.0116\nEpoch [9/100] — Loss: 107.8991\nEpoch [10/100] — Loss: 102.7760\nEpoch [11/100] — Loss: 96.4707\nEpoch [12/100] — Loss: 92.6377\nEpoch [13/100] — Loss: 88.4997\nEpoch [14/100] — Loss: 83.1474\nEpoch [15/100] — Loss: 78.7995\nEpoch [16/100] — Loss: 74.7111\nEpoch [17/100] — Loss: 71.7373\nEpoch [18/100] — Loss: 67.3194\nEpoch [19/100] — Loss: 63.8964\nEpoch [20/100] — Loss: 61.6123\nEpoch [21/100] — Loss: 58.2232\nEpoch [22/100] — Loss: 56.1752\nEpoch [23/100] — Loss: 52.8187\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# 9) Show Trackio","metadata":{}},{"cell_type":"code","source":"trackio.show(project=\"my-training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:31.761798Z","iopub.status.idle":"2025-11-10T07:35:31.762103Z","shell.execute_reply.started":"2025-11-10T07:35:31.761948Z","shell.execute_reply":"2025-11-10T07:35:31.761962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"age_gender_predictor.pth\")\nprint(\"Model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:31.763137Z","iopub.status.idle":"2025-11-10T07:35:31.763373Z","shell.execute_reply.started":"2025-11-10T07:35:31.763275Z","shell.execute_reply":"2025-11-10T07:35:31.763285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10) Take Test and Do Predictions","metadata":{}},{"cell_type":"code","source":"model.eval()\npred_ages, pred_genders, ids = [], [], []\n\nfor i, row in test.iterrows():\n    img_path = os.path.join(base_path, row['full_path'])\n    img = Image.open(img_path).convert(\"RGB\")\n    img = transform(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        age_pred, gender_pred = model(img)\n        pred_ages.append(age_pred.item())                    \n        pred_genders.append(1 if torch.sigmoid(gender_pred) > 0.5 else 0)\n    ids.append(row['id'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:31.764165Z","iopub.status.idle":"2025-11-10T07:35:31.764358Z","shell.execute_reply.started":"2025-11-10T07:35:31.764267Z","shell.execute_reply":"2025-11-10T07:35:31.764276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 11) Fianl Convert and Submit","metadata":{}},{"cell_type":"code","source":"pred_age_int = pd.to_numeric(pred_ages).astype(int)\nsubmission = pd.DataFrame({\n    \"id\": ids,\n    \"age\": pred_age_int,\n    \"gender\": pred_genders\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:31.765238Z","iopub.status.idle":"2025-11-10T07:35:31.765538Z","shell.execute_reply.started":"2025-11-10T07:35:31.765351Z","shell.execute_reply":"2025-11-10T07:35:31.765366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(submission['age'].max())\nprint(submission['age'].min())\n\nprint(submission['id'].max())\nprint(submission['id'].min())\n\nprint(submission['gender'].max())\nprint(submission['gender'].min())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:35:31.766430Z","iopub.status.idle":"2025-11-10T07:35:31.766728Z","shell.execute_reply.started":"2025-11-10T07:35:31.766599Z","shell.execute_reply":"2025-11-10T07:35:31.766617Z"}},"outputs":[],"execution_count":null}]}